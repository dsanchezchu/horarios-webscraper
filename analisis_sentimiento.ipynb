{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrador\\anaconda3\\envs\\bert-es\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Administrador\\anaconda3\\envs\\bert-es\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1072/1072 [00:00<00:00, 4617.65 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 268/268 [00:00<00:00, 4574.56 examples/s]\n",
      "c:\\Users\\Administrador\\anaconda3\\envs\\bert-es\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 134/402 [04:16<09:08,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2157, 'grad_norm': 0.36178359389305115, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 134/402 [04:24<09:08,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06909680366516113, 'eval_runtime': 8.0602, 'eval_samples_per_second': 33.25, 'eval_steps_per_second': 4.218, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/402 [08:40<04:08,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0711, 'grad_norm': 0.018976643681526184, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 268/402 [08:48<04:08,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0814434364438057, 'eval_runtime': 7.3481, 'eval_samples_per_second': 36.472, 'eval_steps_per_second': 4.627, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 402/402 [13:05<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0322, 'grad_norm': 0.010452021844685078, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 402/402 [13:13<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08250705152750015, 'eval_runtime': 7.7553, 'eval_samples_per_second': 34.557, 'eval_steps_per_second': 4.384, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 402/402 [13:18<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 798.1202, 'train_samples_per_second': 4.029, 'train_steps_per_second': 0.504, 'train_loss': 0.10635785321098062, 'epoch': 3.0}\n",
      "üìä Evaluando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:07<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negativo       0.99      0.98      0.98       123\n",
      "    positivo       0.98      0.99      0.99       145\n",
      "\n",
      "    accuracy                           0.99       268\n",
      "   macro avg       0.99      0.98      0.98       268\n",
      "weighted avg       0.99      0.99      0.99       268\n",
      "\n",
      "‚úÖ Modelo y tokenizer guardados en ./modelo_entrenado\n"
     ]
    }
   ],
   "source": [
    "#paso opcional: este codigo no es necesario ejecutar, ya que el entrenamiento del modelo ya esta guardado, \n",
    "# pero si se desea agregar mas informacion a la data para el entrenamiento, se puede volver a entrenar\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "OUTPUT_DIR = \"./modelo_entrenado\"\n",
    "\n",
    "def cargar_datos_entrenamiento():\n",
    "    df = pd.read_csv('comentarios_entrenamiento.csv', sep=';')\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    df['sentimiento'] = df['sentimiento'].str.lower().str.strip()\n",
    "    df = df[df['sentimiento'].isin(['positivo', 'negativo'])]\n",
    "    df['label'] = df['sentimiento'].map({'positivo': 1, 'negativo': 0})\n",
    "    return Dataset.from_pandas(df[['comentario', 'label']])\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"comentario\"], padding=True, truncation=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "    dataset = cargar_datos_entrenamiento()\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"üìä Evaluando modelo...\")\n",
    "    preds = trainer.predict(dataset['test'])\n",
    "    y_pred = preds.predictions.argmax(axis=1)\n",
    "    y_true = preds.label_ids\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"negativo\", \"positivo\"]))\n",
    "\n",
    "    # Guardar modelo entrenado\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"‚úÖ Modelo y tokenizer guardados en {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c50e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clasificaci√≥n generada en 'clasificacion_profesores.csv'\n"
     ]
    }
   ],
   "source": [
    "#¬¥Paso 2: Generacion de clasificacion de profesores\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "MODEL_DIR = \"./modelo_entrenado\"\n",
    "\n",
    "def clasificar_profesores(model, tokenizer):\n",
    "    df = pd.read_csv(\"comentarios/datacoment.csv\", sep=';')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df = df.dropna(subset=['comentarios'])\n",
    "\n",
    "    textos = df['comentarios'].tolist()\n",
    "    tokens = tokenizer(textos, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        preds = torch.argmax(outputs.logits, dim=1).numpy()\n",
    "\n",
    "    df['sentimiento'] = ['positivo' if p == 1 else 'negativo' for p in preds]\n",
    "\n",
    "    clasificacion = []\n",
    "    for profesor, grupo in df.groupby('Docente'):\n",
    "        positivos = (grupo['sentimiento'] == 'positivo').sum()\n",
    "        negativos = (grupo['sentimiento'] == 'negativo').sum()\n",
    "\n",
    "        if positivos > negativos:\n",
    "            clasif = 'bueno'\n",
    "        elif negativos > positivos:\n",
    "            clasif = 'malo'\n",
    "        else:\n",
    "            clasif = 'neutro'\n",
    "\n",
    "        clasificacion.append({'Docente': profesor, 'clasificacion': clasif})\n",
    "\n",
    "    resultado_df = pd.DataFrame(clasificacion)\n",
    "    resultado_df.to_csv(\"clasificacion_profesores.csv\", index=False, sep=';')\n",
    "    print(\"‚úÖ Clasificaci√≥n generada en 'clasificacion_profesores.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "    clasificar_profesores(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9154a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso 3: ejecutar esto cuando se haya extraido todos los comentarios de los profesores, as√≠ se fusionaran con la data de horarios_validos.csv\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# Funci√≥n para normalizar nombres (min√∫sculas, sin tildes, sin espacios extras)\n",
    "def normalizar(nombre):\n",
    "    nombre = unicodedata.normalize('NFKD', nombre).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    return nombre.lower().strip()\n",
    "\n",
    "# Leer archivos\n",
    "horarios = pd.read_csv(\"horarios_validos.csv\", encoding=\"utf-8\")\n",
    "clasificacion = pd.read_csv(\"clasificacion_profesores.csv\", sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "# Normalizar nombres en ambos DataFrames\n",
    "horarios[\"docente_normalizado\"] = horarios[\"Docente\"].apply(normalizar)\n",
    "clasificacion[\"docente_normalizado\"] = clasificacion[\"Docente\"].apply(normalizar)\n",
    "\n",
    "# Hacer el merge por la columna normalizada\n",
    "df_final = horarios.merge(\n",
    "    clasificacion[[\"docente_normalizado\", \"clasificacion\"]],\n",
    "    on=\"docente_normalizado\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Eliminar columna auxiliar y guardar el resultado\n",
    "df_final.drop(columns=[\"docente_normalizado\"], inplace=True)\n",
    "df_final.to_csv(\"horarios_con_clasificacion.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ Archivo 'horarios_con_clasificacion.csv' generado con la columna 'clasificacion'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-es",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
